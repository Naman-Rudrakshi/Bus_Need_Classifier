{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Classification Pipeline**\n",
        "We can infer that the geographic region of the US in question will largely influence the factors that determine school bus need. We want to force our model to choose \"CENSUS_D\" as its first split; the best way to do so is to simply make the split ourselves and train several smaller RFs on each \"split dataset.\""
      ],
      "metadata": {
        "id": "-29du89w1bIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's seperate this data into smaller datasets, differentiated by region (Census Division). We will train a seperate RF on each sub-dataset."
      ],
      "metadata": {
        "id": "iqaOKDzwbvVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "student_trips_filtered = pd.read_csv(\"2017_Final.csv\")"
      ],
      "metadata": {
        "id": "XWa90q97wYRI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of CENSUS_D columns\n",
        "census_d_cols = {\n",
        "    'New England':1,\n",
        "    'Middle Atlantic':2,\n",
        "    'East North Central':3,\n",
        "    'West North Central':4,\n",
        "    'South Atlantic':5,\n",
        "    'East South Central':6,\n",
        "    'West South Central':7,\n",
        "    'Mountain':8,\n",
        "    'Pacific':9\n",
        "}\n",
        "\n",
        "# Dictionary to store mini dataframes\n",
        "mini_dfs = {}\n",
        "\n",
        "# Iterate over each Census D column\n",
        "for col in census_d_cols:\n",
        "    # Select only rows where this category is 1\n",
        "    subset = student_trips_filtered[student_trips_filtered[\"CENSUS_D\"] == census_d_cols[col]].copy()\n",
        "\n",
        "    # Drop all Census D columns\n",
        "    subset = subset.drop(columns=\"CENSUS_D\")\n",
        "\n",
        "    # Store in dictionary\n",
        "    mini_dfs[col] = subset\n",
        "    print(subset.shape)\n",
        "\n",
        "# Now mini_dfs contains a separate DataFrame for each Census D category\n",
        "# Example access:\n",
        "print(\"Number of rows in New England mini-df:\", len(mini_dfs['New England']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBGwT3qC2KiL",
        "outputId": "6137c72b-023f-434d-a7e3-28d34960ef8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(362, 38)\n",
            "(3399, 38)\n",
            "(3029, 38)\n",
            "(1027, 38)\n",
            "(5582, 38)\n",
            "(296, 38)\n",
            "(6154, 38)\n",
            "(975, 38)\n",
            "(5351, 38)\n",
            "Number of rows in New England mini-df: 362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train a Random Forest on each of these mini datasets. We can use a threshold of 0.40 instead of 0.50 (to prioritize recall over precision, since it's better to falsely predict a kid needs a bus than falsely predict that they don't)."
      ],
      "metadata": {
        "id": "Kb7GcamiFdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, accuracy_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Assume mini_dfs is the dictionary from your previous step\n",
        "target_col = 'NEEDS_SCHOOL_BUS'\n",
        "\n",
        "# Dictionary to store trained Random Forests\n",
        "rf_models = {}\n",
        "\n",
        "# Dictionary to store metrics\n",
        "rf_metrics = {}\n",
        "\n",
        "for census_cat, df in mini_dfs.items():\n",
        "    print(f\"\\n=== Processing {census_cat} ===\")\n",
        "\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Train-test split 80-20\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Train RF\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Store model\n",
        "    rf_models[census_cat] = rf\n",
        "\n",
        "    # Predictions\n",
        "    y_prob = rf.predict_proba(X_test)[:, 1]  # for ROC-AUC\n",
        "    y_pred = (y_prob >= 0.40).astype(int)   # apply threshold of 0.40\n",
        "\n",
        "    train_prob = rf.predict_proba(X_train)[:, 1]\n",
        "    print(\"Train ROC-AUC:\", roc_auc_score(y_train, train_prob))\n",
        "    print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision_0': precision_score(y_test, y_pred, pos_label=0, zero_division=0),\n",
        "        'precision_1': precision_score(y_test, y_pred, pos_label=1, zero_division=0),\n",
        "        'recall_0': recall_score(y_test, y_pred, pos_label=0, zero_division=0),\n",
        "        'recall_1': recall_score(y_test, y_pred, pos_label=1, zero_division=0),\n",
        "        'f1_0': f1_score(y_test, y_pred, pos_label=0, zero_division=0),\n",
        "        'f1_1': f1_score(y_test, y_pred, pos_label=1, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_test, y_prob),\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "        \"y_pred_array\": y_pred,\n",
        "        \"y_test_array\": y_test.values,\n",
        "        \"y_prob_array\": y_prob,\n",
        "        \"y_train_array\": y_train.values,\n",
        "        \"y_train_prob_array\": rf.predict_proba(X_train)[:, 1]\n",
        "    }\n",
        "\n",
        "    rf_metrics[census_cat] = metrics\n",
        "\n",
        "    # Display metrics\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
        "    print(f\"ROC-AUC: {metrics['roc_auc']:.3f}\")\n",
        "    print(f\"Precision: 0={metrics['precision_0']:.3f}, 1={metrics['precision_1']:.3f}\")\n",
        "    print(f\"Recall: 0={metrics['recall_0']:.3f}, 1={metrics['recall_1']:.3f}\")\n",
        "    print(f\"F1-score: 0={metrics['f1_0']:.3f}, 1={metrics['f1_1']:.3f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(metrics['confusion_matrix'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5R8iQr34bUI",
        "outputId": "ba54f8e3-6716-4f2c-b256-f23bef724446"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing New England ===\n",
            "Train ROC-AUC: 1.0\n",
            "Test ROC-AUC: 0.6733993902439024\n",
            "Accuracy: 0.603\n",
            "ROC-AUC: 0.673\n",
            "Precision: 0=0.658, 1=0.543\n",
            "Recall: 0=0.610, 1=0.594\n",
            "F1-score: 0=0.633, 1=0.567\n",
            "Confusion Matrix:\n",
            "[[25 16]\n",
            " [13 19]]\n",
            "\n",
            "=== Processing Middle Atlantic ===\n",
            "Train ROC-AUC: 0.9993013203051566\n",
            "Test ROC-AUC: 0.7857729544580756\n",
            "Accuracy: 0.719\n",
            "ROC-AUC: 0.786\n",
            "Precision: 0=0.765, 1=0.698\n",
            "Recall: 0=0.542, 1=0.864\n",
            "F1-score: 0=0.635, 1=0.772\n",
            "Confusion Matrix:\n",
            "[[166 140]\n",
            " [ 51 323]]\n",
            "\n",
            "=== Processing East North Central ===\n",
            "Train ROC-AUC: 0.9979327649091615\n",
            "Test ROC-AUC: 0.8144933435494683\n",
            "Accuracy: 0.754\n",
            "ROC-AUC: 0.814\n",
            "Precision: 0=0.823, 1=0.639\n",
            "Recall: 0=0.792, 1=0.684\n",
            "F1-score: 0=0.807, 1=0.661\n",
            "Confusion Matrix:\n",
            "[[312  82]\n",
            " [ 67 145]]\n",
            "\n",
            "=== Processing West North Central ===\n",
            "Train ROC-AUC: 0.9996442255062945\n",
            "Test ROC-AUC: 0.7997835497835498\n",
            "Accuracy: 0.791\n",
            "ROC-AUC: 0.800\n",
            "Precision: 0=0.849, 1=0.672\n",
            "Recall: 0=0.843, 1=0.682\n",
            "F1-score: 0=0.846, 1=0.677\n",
            "Confusion Matrix:\n",
            "[[118  22]\n",
            " [ 21  45]]\n",
            "\n",
            "=== Processing South Atlantic ===\n",
            "Train ROC-AUC: 0.9995909590635711\n",
            "Test ROC-AUC: 0.8076118497687531\n",
            "Accuracy: 0.755\n",
            "ROC-AUC: 0.808\n",
            "Precision: 0=0.810, 1=0.663\n",
            "Recall: 0=0.799, 1=0.679\n",
            "F1-score: 0=0.805, 1=0.671\n",
            "Confusion Matrix:\n",
            "[[564 142]\n",
            " [132 279]]\n",
            "\n",
            "=== Processing East South Central ===\n",
            "Train ROC-AUC: 0.9959284627092847\n",
            "Test ROC-AUC: 0.862514688601645\n",
            "Accuracy: 0.750\n",
            "ROC-AUC: 0.863\n",
            "Precision: 0=0.867, 1=0.633\n",
            "Recall: 0=0.703, 1=0.826\n",
            "F1-score: 0=0.776, 1=0.717\n",
            "Confusion Matrix:\n",
            "[[26 11]\n",
            " [ 4 19]]\n",
            "\n",
            "=== Processing West South Central ===\n",
            "Train ROC-AUC: 0.9993363880904921\n",
            "Test ROC-AUC: 0.7890725594711439\n",
            "Accuracy: 0.812\n",
            "ROC-AUC: 0.789\n",
            "Precision: 0=0.857, 1=0.632\n",
            "Recall: 0=0.901, 1=0.530\n",
            "F1-score: 0=0.879, 1=0.577\n",
            "Confusion Matrix:\n",
            "[[841  92]\n",
            " [140 158]]\n",
            "\n",
            "=== Processing Mountain ===\n",
            "Train ROC-AUC: 0.9993056154914437\n",
            "Test ROC-AUC: 0.8338786810974074\n",
            "Accuracy: 0.779\n",
            "ROC-AUC: 0.834\n",
            "Precision: 0=0.846, 1=0.627\n",
            "Recall: 0=0.839, 1=0.638\n",
            "F1-score: 0=0.842, 1=0.632\n",
            "Confusion Matrix:\n",
            "[[115  22]\n",
            " [ 21  37]]\n",
            "\n",
            "=== Processing Pacific ===\n",
            "Train ROC-AUC: 0.9996499465220203\n",
            "Test ROC-AUC: 0.8179743935558964\n",
            "Accuracy: 0.915\n",
            "ROC-AUC: 0.818\n",
            "Precision: 0=0.930, 1=0.672\n",
            "Recall: 0=0.978, 1=0.381\n",
            "F1-score: 0=0.954, 1=0.486\n",
            "Confusion Matrix:\n",
            "[[937  21]\n",
            " [ 70  43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's figure out the overall metrics. We need special logic to combine the metrics of each individual RF for every Census Division into an \"overall\" set of metrics."
      ],
      "metadata": {
        "id": "0KigmcqUHGJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_stats(rf_metrics):\n",
        "\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    # Initialize accumulators\n",
        "    total_samples = 0\n",
        "    weighted_acc = 0\n",
        "    weighted_precision_0 = 0\n",
        "    weighted_precision_1 = 0\n",
        "    weighted_recall_0 = 0\n",
        "    weighted_recall_1 = 0\n",
        "    weighted_f1_0 = 0\n",
        "    weighted_f1_1 = 0\n",
        "    total_conf_matrix = np.zeros((2,2), dtype=int)\n",
        "\n",
        "    all_y_test = []\n",
        "    all_y_prob = []\n",
        "    all_y_train = []\n",
        "    all_y_train_prob = []\n",
        "\n",
        "    for cat, metrics in rf_metrics.items():\n",
        "        n = metrics['confusion_matrix'].sum()  # total samples in this mini-test\n",
        "        total_samples += n\n",
        "\n",
        "        # Accuracy weighted\n",
        "        weighted_acc += metrics['accuracy'] * n\n",
        "\n",
        "        # Precision/recall/f1 weighted by number of class instances\n",
        "        cm = metrics['confusion_matrix']\n",
        "        n0 = cm[0,0] + cm[0,1]  # true class 0\n",
        "        n1 = cm[1,0] + cm[1,1]  # true class 1\n",
        "\n",
        "        weighted_precision_0 += metrics['precision_0'] * n0\n",
        "        weighted_precision_1 += metrics['precision_1'] * n1\n",
        "        weighted_recall_0 += metrics['recall_0'] * n0\n",
        "        weighted_recall_1 += metrics['recall_1'] * n1\n",
        "        weighted_f1_0 += metrics['f1_0'] * n0\n",
        "        weighted_f1_1 += metrics['f1_1'] * n1\n",
        "\n",
        "        # Confusion matrix sum\n",
        "        total_conf_matrix += cm\n",
        "\n",
        "        # Collect probs for ROC-AUC\n",
        "        all_y_test.append(metrics.get('y_test_array'))\n",
        "        all_y_prob.append(metrics.get('y_prob_array'))\n",
        "        all_y_train.append(metrics.get('y_train_array'))\n",
        "        all_y_train_prob.append(metrics.get('y_train_prob_array'))\n",
        "\n",
        "    # Collect total class counts\n",
        "    total_n0 = sum(m['confusion_matrix'][0,0]+m['confusion_matrix'][0,1] for m in rf_metrics.values())\n",
        "    total_n1 = sum(m['confusion_matrix'][1,0]+m['confusion_matrix'][1,1] for m in rf_metrics.values())\n",
        "\n",
        "    overall_metrics = {\n",
        "        'accuracy': weighted_acc / total_samples,\n",
        "        'precision_0': weighted_precision_0 / total_n0,\n",
        "        'precision_1': weighted_precision_1 / total_n1,\n",
        "        'recall_0': weighted_recall_0 / total_n0,\n",
        "        'recall_1': weighted_recall_1 / total_n1,\n",
        "        'f1_0': weighted_f1_0 / total_n0,\n",
        "        'f1_1': weighted_f1_1 / total_n1,\n",
        "        'train_roc_auc': roc_auc_score(np.concatenate(all_y_train), np.concatenate(all_y_train_prob)),\n",
        "        'test_roc_auc': roc_auc_score(np.concatenate(all_y_test), np.concatenate(all_y_prob)),\n",
        "        'confusion_matrix': total_conf_matrix\n",
        "    }\n",
        "\n",
        "    for i in overall_metrics:\n",
        "        print(i, \":\", overall_metrics[i])\n"
      ],
      "metadata": {
        "id": "pnBzXLdvHIr6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check metrics."
      ],
      "metadata": {
        "id": "IUMq8M4wBn3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_stats(rf_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tr14SFmXMax",
        "outputId": "5ab7759a-1933-4a86-d1af-1db890064847"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.7963351784691735\n",
            "precision_0 : 0.853101273912301\n",
            "precision_1 : 0.6588532329584507\n",
            "recall_0 : 0.8499452354874042\n",
            "recall_1 : 0.6729678638941399\n",
            "f1_0 : 0.8495014759753476\n",
            "f1_1 : 0.6597829347108644\n",
            "train_roc_auc : 0.9993429089649566\n",
            "test_roc_auc : 0.8374317859166518\n",
            "confusion_matrix : [[3104  548]\n",
            " [ 519 1068]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def geocode_address_census(address):\n",
        "    url = \"https://geocoding.geo.census.gov/geocoder/locations/onelineaddress\"\n",
        "\n",
        "    params = {\n",
        "        \"address\": address,\n",
        "        \"benchmark\": \"Public_AR_Current\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params).json()\n",
        "    matches = response[\"result\"][\"addressMatches\"]\n",
        "\n",
        "    if len(matches) == 0:\n",
        "        raise ValueError(\"Address not found.\")\n",
        "\n",
        "    coords = matches[0][\"coordinates\"]\n",
        "    return coords[\"y\"], coords[\"x\"]   # (lat, lon)\n"
      ],
      "metadata": {
        "id": "gEQRIGfVMN_V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "State, county, tract, block group identifiers based on coordinates."
      ],
      "metadata": {
        "id": "d0KonMdR3Isu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fips_from_coords(lat, lon):\n",
        "    \"\"\"\n",
        "    Given lat/lon, return:\n",
        "    - state FIPS\n",
        "    - county FIPS\n",
        "    - tract code\n",
        "    - block group code\n",
        "    \"\"\"\n",
        "    url = f\"https://geo.fcc.gov/api/census/block/find?latitude={lat}&longitude={lon}&format=json\"\n",
        "    response = requests.get(url).json()\n",
        "\n",
        "    block_fips = response[\"Block\"][\"FIPS\"]  # 15-digit block code\n",
        "\n",
        "    state_fips = block_fips[:2]       # 2 digits\n",
        "    county_fips = block_fips[2:5]     # 3 digits\n",
        "    tract = block_fips[5:11]          # 6 digits\n",
        "    block = block_fips[11:]           # 4 digits\n",
        "    block_group = block[0]            # 1 digit\n",
        "\n",
        "    return state_fips, county_fips, tract, block_group\n"
      ],
      "metadata": {
        "id": "QcGSObIs3ITo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Census division."
      ],
      "metadata": {
        "id": "MOWIGXQT4d9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_TO_DIVISION = {\n",
        "    # New England\n",
        "    \"09\":\"New England\",\"23\":\"New England\",\"25\":\"New England\",\"33\":\"New England\",\"44\":\"New England\",\"50\":\"New England\",\n",
        "\n",
        "    # Middle Atlantic\n",
        "    \"34\":\"Middle Atlantic\",\"36\":\"Middle Atlantic\",\"42\":\"Middle Atlantic\",\n",
        "\n",
        "    # East North Central\n",
        "    \"17\":\"East North Central\",\"18\":\"East North Central\",\"26\":\"East North Central\",\"39\":\"East North Central\",\"55\":\"East North Central\",\n",
        "\n",
        "    # West North Central\n",
        "    \"19\":\"West North Central\",\"20\":\"West North Central\",\"27\":\"West North Central\",\"29\":\"West North Central\",\n",
        "    \"31\":\"West North Central\",\"38\":\"West North Central\",\"46\":\"West North Central\",\n",
        "\n",
        "    # South Atlantic\n",
        "    \"10\":\"South Atlantic\",\"11\":\"South Atlantic\",\"12\":\"South Atlantic\",\"13\":\"South Atlantic\",\n",
        "    \"24\":\"South Atlantic\",\"37\":\"South Atlantic\",\"45\":\"South Atlantic\",\"51\":\"South Atlantic\",\"54\":\"South Atlantic\",\n",
        "\n",
        "    # East South Central\n",
        "    \"01\":\"East South Central\",\"21\":\"East South Central\",\"28\":\"East South Central\",\"47\":\"East South Central\",\n",
        "\n",
        "    # West South Central\n",
        "    \"05\":\"West South Central\",\"22\":\"West South Central\",\"40\":\"West South Central\",\"48\":\"West South Central\",\n",
        "\n",
        "    # Mountain\n",
        "    \"04\":\"Mountain\",\"08\":\"Mountain\",\"16\":\"Mountain\",\"30\":\"Mountain\",\n",
        "    \"32\":\"Mountain\",\"35\":\"Mountain\",\"49\":\"Mountain\",\"56\":\"Mountain\",\n",
        "\n",
        "    # Pacific\n",
        "    \"02\":\"Pacific\",\"06\":\"Pacific\",\"15\":\"Pacific\",\"41\":\"Pacific\",\"53\":\"Pacific\"\n",
        "}\n",
        "\n",
        "def get_division(state_fips):\n",
        "  return STATE_TO_DIVISION[state_fips]"
      ],
      "metadata": {
        "id": "FI-00e_J4fSQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSA status and code."
      ],
      "metadata": {
        "id": "ZsF17dD93d9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_MSA_status(state_fips, county_fips):\n",
        "  url = \"https://www2.census.gov/programs-surveys/metro-micro/geographies/reference-files/2023/delineation-files/list1_2023.xlsx\"\n",
        "  cbsa_crosswalk = pd.read_excel(url, dtype=str,header=2)\n",
        "  val = cbsa_crosswalk[(cbsa_crosswalk[\"FIPS State Code\"] == state_fips) & (cbsa_crosswalk[\"FIPS County Code\"] == county_fips)]\n",
        "  return val[\"CBSA Code\"].tolist()[0], val[\"Metropolitan/Micropolitan Statistical Area\"].tolist()[0]\n"
      ],
      "metadata": {
        "id": "eHDJiS_r3mjz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Urban/Rural classification by coordinates."
      ],
      "metadata": {
        "id": "j1ZIdmFB4SGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Configuration: URL of shapefile ZIP\n",
        "UAC20_URL = \"https://www2.census.gov/geo/tiger/TIGER2020/UAC/tl_2020_us_uac20.zip\"\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Helper to load shapefile into GeoDataFrame\n",
        "def load_urban_areas_gdf(url=UAC20_URL):\n",
        "    # Download zip into bytes\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "    # Find the .shp file name inside the zip\n",
        "    shapefile_name = [f for f in z.namelist() if f.endswith(\".shp\")][0]\n",
        "\n",
        "    # Extract all files into memory buffer\n",
        "    z.extractall(\"/tmp/tl_uac20\")\n",
        "\n",
        "    # Load with GeoPandas\n",
        "    gdf = gpd.read_file(f\"/tmp/tl_uac20/{shapefile_name}\")\n",
        "    # Ensure it's in WGS84 lat/lon\n",
        "    gdf = gdf.to_crs(epsg=4326)\n",
        "    return gdf\n",
        "\n",
        "# Load once\n",
        "urban_gdf = load_urban_areas_gdf()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "def classify_urban(lat, lon, gdf=urban_gdf):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - 'Urban' if the point is inside any urban polygon\n",
        "      - 'Rural' otherwise\n",
        "      - urban area name if inside urban, else None\n",
        "    \"\"\"\n",
        "    pt = Point(lon, lat)\n",
        "    match = gdf[gdf.contains(pt)]\n",
        "    if not match.empty:\n",
        "        # Inside some urban polygon\n",
        "        name = match.iloc[0][\"NAME20\"]\n",
        "        return \"Urban\", name\n",
        "    else:\n",
        "        return \"Rural\", None  # Not in urban area"
      ],
      "metadata": {
        "id": "zylCq10K4URH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Miles to school."
      ],
      "metadata": {
        "id": "_bkBxvsh4soM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get a free key from https://openrouteservice.org/sign-up/\n",
        "API_KEY = \"eyJvcmciOiI1YjNjZTM1OTc4NTExMTAwMDFjZjYyNDgiLCJpZCI6ImFiMTU3YmFjMzYxNzQ3MGRhZGY5ZWQ4MTFmOTE0ZGZiIiwiaCI6Im11cm11cjY0In0=\"\n",
        "\n",
        "def get_driving_distance_ors(address1, address2):\n",
        "    \"\"\"\n",
        "    Returns driving distance in kilometers and duration in minutes using OpenRouteService.\n",
        "    \"\"\"\n",
        "    # First, geocode addresses using ORS\n",
        "    def geocode(address):\n",
        "        url = \"https://api.openrouteservice.org/geocode/search\"\n",
        "        params = {\"api_key\": API_KEY, \"text\": address, \"size\": 1}\n",
        "        resp = requests.get(url, params=params).json()\n",
        "        if len(resp[\"features\"]) == 0:\n",
        "            raise ValueError(f\"Address not found: {address}\")\n",
        "        coords = resp[\"features\"][0][\"geometry\"][\"coordinates\"]  # [lon, lat]\n",
        "        return coords\n",
        "\n",
        "    start_coords = geocode(address1)\n",
        "    end_coords = geocode(address2)\n",
        "\n",
        "    # Call directions endpoint\n",
        "    url = \"https://api.openrouteservice.org/v2/directions/driving-car\"\n",
        "    headers = {\"Authorization\": API_KEY, \"Content-Type\": \"application/json\"}\n",
        "    body = {\n",
        "        \"coordinates\": [start_coords, end_coords]\n",
        "    }\n",
        "    resp = requests.post(url, json=body, headers=headers).json()\n",
        "    route = resp[\"routes\"][0][\"summary\"]\n",
        "    distance_mi = route[\"distance\"] / 1000 * 0.621371\n",
        "    duration_min = route[\"duration\"] / 60\n",
        "\n",
        "    return distance_mi, duration_min"
      ],
      "metadata": {
        "id": "y4h8yyXyQMGQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Median income, race/Hispanic counts by block group."
      ],
      "metadata": {
        "id": "6YSJ97_P0EBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_census_block_group_data(state_fips, county_fips, tract, block_group, api_key=None):\n",
        "    \"\"\"\n",
        "    Fetch block group median income, race/Hispanic counts\n",
        "    in one function call.\n",
        "\n",
        "    Parameters:\n",
        "        state_fips (str): 2-digit state FIPS\n",
        "        county_fips (str): 3-digit county FIPS\n",
        "        tract (str): 6-digit tract code\n",
        "        block_group (str): 1-digit block group code\n",
        "        api_key (str, optional): Your Census API key\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"median_income\": int or \"No data\",\n",
        "            \"race_counts\": dict of race/Hispanic counts,\n",
        "        }\n",
        "    \"\"\"\n",
        "    # ---------- Block group: median income + race/Hispanic ----------\n",
        "    base_bg = \"https://api.census.gov/data/2022/acs/acs5\"\n",
        "\n",
        "    race_vars = [\n",
        "        \"B02001_002E\",  # White\n",
        "        \"B02001_003E\",  # Black or African American\n",
        "        \"B02001_004E\",  # Asian\n",
        "        \"B02001_005E\",  # American Indian or Alaska Native\n",
        "        \"B02001_006E\",  # Native Hawaiian or other Pacific Islander\n",
        "        \"B02001_007E\",  # Other race\n",
        "        \"B02001_008E\",  # Two or more races\n",
        "    ]\n",
        "    hisp_vars = [\n",
        "        \"B03003_002E\",  # Not Hispanic\n",
        "        \"B03003_003E\"   # Hispanic\n",
        "    ]\n",
        "\n",
        "    all_vars = [\"B19013_001E\"] + race_vars + hisp_vars\n",
        "    var_str = \",\".join(all_vars)\n",
        "\n",
        "    params_bg = {\n",
        "        \"get\": var_str,\n",
        "        \"for\": f\"block group:{block_group}\",\n",
        "        \"in\": f\"state:{state_fips}+county:{county_fips}+tract:{tract}\"\n",
        "    }\n",
        "    if api_key:\n",
        "        params_bg[\"key\"] = api_key\n",
        "\n",
        "    response_bg = requests.get(base_bg, params=params_bg)\n",
        "    response_bg.raise_for_status()\n",
        "    data_bg = response_bg.json()\n",
        "\n",
        "    # Median income\n",
        "    median_income_raw = data_bg[1][0]\n",
        "    median_income = int(median_income_raw) if median_income_raw not in [None, \"\", \"null\"] else \"No data\"\n",
        "\n",
        "    # Race/Hispanic counts\n",
        "    counts_raw = dict(zip(data_bg[0][1:], data_bg[1][1:]))  # skip median income key\n",
        "    race_counts = {k: int(v) for k, v in counts_raw.items()}\n",
        "\n",
        "\n",
        "    # ---------- Combine results ----------\n",
        "    return {\n",
        "        \"median_income\": median_income,\n",
        "        \"race_counts\": race_counts,\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "QV_acj1o6GPE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBSA population."
      ],
      "metadata": {
        "id": "SM4b2xmBV1DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_census_CBSA_data(state_fips, county_fips, api_key=None):\n",
        "\n",
        "    # ---------- Block group: median income + race/Hispanic ----------\n",
        "    base_bg = \"https://api.census.gov/data/2022/acs/acs5\"\n",
        "\n",
        "    # ---------- CBSA population ----------\n",
        "    cbsa_code, msa_status = get_MSA_status(state_fips, county_fips)\n",
        "    if \"Micropolitan\" in msa_status:\n",
        "      return \"Not in MSA/CMSA\"\n",
        "\n",
        "    base_cbsa = \"https://api.census.gov/data/2022/acs/acs5\"\n",
        "    params_cbsa = {\n",
        "        \"get\": \"B01003_001E\",\n",
        "        \"for\": f\"metropolitan statistical area/micropolitan statistical area:{cbsa_code}\"\n",
        "    }\n",
        "    if api_key:\n",
        "        params_cbsa[\"key\"] = api_key\n",
        "\n",
        "    response_cbsa = requests.get(base_cbsa, params=params_cbsa)\n",
        "    response_cbsa.raise_for_status()\n",
        "    data_cbsa = response_cbsa.json()\n",
        "    cbsa_population = int(data_cbsa[1][0])\n",
        "\n",
        "\n",
        "    return cbsa_population\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zXbnjl-nV2Wm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also retrieve tract-level data for # workers, % rented housing, # housing units, and total population for the respective columns."
      ],
      "metadata": {
        "id": "_qGzV0QFUewl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_census_tract_data(state_fips, county_fips, tract, api_key=None):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "      - workers_per_sq_mile\n",
        "      - pct_renter\n",
        "      - population_density\n",
        "      - housing_units_per_sq_mile\n",
        "      - tract_land_area_sqmi (filled later)\n",
        "    \"\"\"\n",
        "\n",
        "    base = \"https://api.census.gov/data/2022/acs/acs5\"\n",
        "\n",
        "    tract_vars = [\n",
        "        \"B23025_003E\", \"B23025_006E\",   # Employed male, employed female\n",
        "        \"B25003_002E\", \"B25003_003E\",   # Owner occ, renter occ\n",
        "        \"B01003_001E\",                  # Total population\n",
        "        \"B25001_001E\"                   # Housing units\n",
        "    ]\n",
        "\n",
        "    params_tract = {\n",
        "        \"get\": \",\".join(tract_vars),\n",
        "        \"for\": f\"tract:{tract}\",\n",
        "        \"in\": f\"state:{state_fips}+county:{county_fips}\"\n",
        "    }\n",
        "    if api_key:\n",
        "        params_tract[\"key\"] = api_key\n",
        "\n",
        "    tract_data = requests.get(base, params=params_tract).json()\n",
        "    trow = tract_data[1]\n",
        "\n",
        "    emp_male  = int(trow[0])\n",
        "    emp_female = int(trow[1])\n",
        "    owner_occ  = int(trow[2])\n",
        "    renter_occ = int(trow[3])\n",
        "    pop_total  = int(trow[4])\n",
        "    housing_units = int(trow[5])\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"tract_workers\": emp_male + emp_female,\n",
        "        \"percent_renter_occupied\": renter_occ/(owner_occ+renter_occ),\n",
        "        \"tract_population\": pop_total,\n",
        "        \"tract_housing_units\": housing_units,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "z_4itSV-UjDB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to retreive the tract-level land area."
      ],
      "metadata": {
        "id": "yu2-C7HkWW6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_tract_land_area(state_fips, county_fips, tract, api_key=None):\n",
        "    \"\"\"\n",
        "    Return land area (m² and sq mi) for the given census tract\n",
        "    using GEOINFO 2023 dataset.\n",
        "    \"\"\"\n",
        "    base = \"https://api.census.gov/data/2023/geoinfo\"\n",
        "\n",
        "    params = {\n",
        "        \"get\": \"AREALAND,AREALAND_SQMI\",\n",
        "        \"for\": f\"tract:{tract}\",\n",
        "        \"in\": f\"state:{state_fips}+county:{county_fips}\"\n",
        "    }\n",
        "    if api_key:\n",
        "        params[\"key\"] = api_key\n",
        "\n",
        "    resp = requests.get(base, params=params)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "\n",
        "    if len(data) < 2:\n",
        "        raise ValueError(\"No geography data returned for tract\")\n",
        "\n",
        "    land_m2 = float(data[1][0])\n",
        "    land_sqmi = float(data[1][1])\n",
        "\n",
        "    return {\"land_area_m2\": land_m2, \"land_area_sqmi\": land_sqmi}\n"
      ],
      "metadata": {
        "id": "0UyvI_3_WrNn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything we need. Let's create some logic to map everything properly. We need to map all of our API call results to the right values in these columns:\n",
        "['DTEEMPDN', 'DTHTNRNT', 'DTPPOPDN', 'DTRESDN', 'OTEEMPDN', 'OTHTNRNT',\n",
        "       'OTPPOPDN', 'OTRESDN', 'CENSUS_D', 'URBRUR', 'NEEDS_SCHOOL_BUS',\n",
        "       'HHFAMINC_Less than $10,000', 'HHFAMINC_$10,000 to $14,999',\n",
        "       'HHFAMINC_$15,000 to $24,999', 'HHFAMINC_$25,000 to $34,999',\n",
        "       'HHFAMINC_$35,000 to $49,999', 'HHFAMINC_$50,000 to $74,999',\n",
        "       'HHFAMINC_$75,000 to $99,999', 'HHFAMINC_$100,000 to $124,999',\n",
        "       'HHFAMINC_$125,000 to $149,999', 'HHFAMINC_$150,000 to $199,999',\n",
        "       'HHFAMINC_$200,000 or more', 'HH_RACE_White',\n",
        "       'HH_RACE_Black or African American', 'HH_RACE_Asian',\n",
        "       'HH_RACE_American Indian or Alaska Native',\n",
        "       'HH_RACE_Native Hawaiian or other Pacific Islander',\n",
        "       'MSASIZE_In an MSA of Less than 250,000',\n",
        "       'MSASIZE_In an MSA of 250,000 - 499,999',\n",
        "       'MSASIZE_In an MSA of 500,000 - 999,999',\n",
        "       'MSASIZE_In an MSA or CMSA of 1,000,000 - 2,999,999',\n",
        "       'MSASIZE_In an MSA or CMSA of 3 million or more',\n",
        "       'MSASIZE_Not in MSA or CMSA', 'HHFAMINC_No data', 'HH_RACE_Other',\n",
        "       'HH_RACE_No data', 'HH_HISP_No data', 'HH_HISP', 'LOG_DIST']"
      ],
      "metadata": {
        "id": "OQ2qFBW--64N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def vector_generator(home_address, school_address):\n",
        "  final_vector = {}\n",
        "  hv = {}\n",
        "\n",
        "  lat, lon = geocode_address_census(home_address)\n",
        "  state_fips, county_fips, tract, block_group = get_fips_from_coords(lat,lon)\n",
        "  hv[\"Home Latitude\"], hv[\"Home Longitude\"], hv[\"Home State FIPS Code\"], hv[\"Home County FIPS Code\"], hv[\"Home Tract Number\"], hv[\"Home Block Group Number\"] = lat, lon, state_fips, county_fips, tract, block_group\n",
        "\n",
        "  slat, slon = geocode_address_census(school_address)\n",
        "  sstate_fips, scounty_fips, stract, sblock_group = get_fips_from_coords(slat,slon)\n",
        "  hv[\"School Latitude\"], hv[\"School Longitude\"], hv[\"School State FIPS Code\"], hv[\"School County FIPS Code\"], hv[\"School Tract Number\"], hv[\"School Block Group Number\"] = slat, slon, sstate_fips, scounty_fips, stract, sblock_group\n",
        "\n",
        "\n",
        "  #school distance\n",
        "  mi, min = get_driving_distance_ors(home_address, school_address)\n",
        "\n",
        "  final_vector[\"LOG_DIST\"] = math.log(mi)\n",
        "  hv[\"Distance to School\"] = mi\n",
        "\n",
        "  #census division\n",
        "  final_vector[\"CENSUS_D\"] = STATE_TO_DIVISION[state_fips]\n",
        "  hv[\"Home Census Division\"] = final_vector[\"CENSUS_D\"]\n",
        "\n",
        "  #CBSA code\n",
        "  hv[\"Home CBSA Code\"] = get_MSA_status(state_fips, county_fips)[0]\n",
        "\n",
        "  #census data\n",
        "\n",
        "  #race, hisp, and median income\n",
        "  census_data = get_census_block_group_data(state_fips, county_fips, tract, block_group)\n",
        "\n",
        "  # --- Race data ---\n",
        "  race_counts = census_data[\"race_counts\"]\n",
        "\n",
        "  # Extract individual race counts (default to 0 if missing)\n",
        "  white = race_counts.get(\"B02001_002E\", 0)\n",
        "  black = race_counts.get(\"B02001_003E\", 0)\n",
        "  asian = race_counts.get(\"B02001_004E\", 0)\n",
        "  native_american = race_counts.get(\"B02001_005E\", 0)\n",
        "  pacific_islander = race_counts.get(\"B02001_006E\", 0)\n",
        "  other = race_counts.get(\"B02001_007E\", 0) + race_counts.get(\"B02001_008E\", 0)\n",
        "\n",
        "  # --- One-hot column names to actual human-readable labels ---\n",
        "  race_label_map = {\n",
        "      \"HH_RACE_White\": \"White\",\n",
        "      \"HH_RACE_Black or African American\": \"Black or African American\",\n",
        "      \"HH_RACE_Asian\": \"Asian\",\n",
        "      \"HH_RACE_American Indian or Alaska Native\": \"American Indian or Alaska Native\",\n",
        "      \"HH_RACE_Native Hawaiian or other Pacific Islander\": \"Native Hawaiian or other Pacific Islander\",\n",
        "      \"HH_RACE_Other\": \"Other\",\n",
        "      \"HH_RACE_No data\": \"No data\"\n",
        "  }\n",
        "\n",
        "  # If everything is missing → mark No Data\n",
        "  if sum([white, black, asian, native_american, pacific_islander, other]) == 0:\n",
        "      for key in race_label_map:\n",
        "          final_vector[key] = 1 if key == \"HH_RACE_No data\" else 0\n",
        "\n",
        "      hv[\"Estimated Race\"] = \"No data\"\n",
        "\n",
        "  else:\n",
        "      # Determine dominant race by max count\n",
        "      race_mapping = {\n",
        "          \"HH_RACE_White\": white,\n",
        "          \"HH_RACE_Black or African American\": black,\n",
        "          \"HH_RACE_Asian\": asian,\n",
        "          \"HH_RACE_American Indian or Alaska Native\": native_american,\n",
        "          \"HH_RACE_Native Hawaiian or other Pacific Islander\": pacific_islander,\n",
        "          \"HH_RACE_Other\": other\n",
        "      }\n",
        "\n",
        "      top_race_key = max(race_mapping, key=race_mapping.get)\n",
        "      top_race_label = race_label_map[top_race_key]\n",
        "\n",
        "      # One-hot encoding\n",
        "      for key in race_label_map:\n",
        "          final_vector[key] = 1 if key == top_race_key else 0\n",
        "\n",
        "      # Assign to human readable vector\n",
        "      hv[\"Block Group Mode Race\"] = top_race_label\n",
        "\n",
        "  #hisp data\n",
        "  # 003E = Hispanic, 002E = Not Hispanic\n",
        "  hisp_counts = race_counts\n",
        "  if hisp_counts.get(\"B03003_003E\", 0) > hisp_counts.get(\"B03003_002E\", 0):\n",
        "      final_vector[\"HH_HISP\"] = 1\n",
        "      final_vector[\"HH_HISP_No data\"] = 0\n",
        "      hv[\"Block Group Mode Hispanic\"] = \"Hispanic\"\n",
        "  elif hisp_counts.get(\"B03003_003E\", 0) <= hisp_counts.get(\"B03003_002E\", 0):\n",
        "      final_vector[\"HH_HISP\"] = 0\n",
        "      final_vector[\"HH_HISP_No data\"] = 0\n",
        "      hv[\"Block Group Mode Hispanic\"] = \"Not Hispanic\"\n",
        "  else:\n",
        "      final_vector[\"HH_HISP_No data\"] = 1\n",
        "      hv[\"Block Group Mode Hispanic\"] = \"No data\"\n",
        "\n",
        "\n",
        "  # --- Median Household Income (one-hot) ---\n",
        "  median_income = census_data.get(\"median_income\")\n",
        "\n",
        "  income_cols = [\n",
        "      'HHFAMINC_Less than $10,000',\n",
        "      'HHFAMINC_$10,000 to $14,999',\n",
        "      'HHFAMINC_$15,000 to $24,999',\n",
        "      'HHFAMINC_$25,000 to $34,999',\n",
        "      'HHFAMINC_$35,000 to $49,999',\n",
        "      'HHFAMINC_$50,000 to $74,999',\n",
        "      'HHFAMINC_$75,000 to $99,999',\n",
        "      'HHFAMINC_$100,000 to $124,999',\n",
        "      'HHFAMINC_$125,000 to $149,999',\n",
        "      'HHFAMINC_$150,000 to $199,999',\n",
        "      'HHFAMINC_$200,000 or more',\n",
        "      'HHFAMINC_No data'\n",
        "  ]\n",
        "\n",
        "  # initialize all income one-hot cols to 0\n",
        "  for col in income_cols:\n",
        "      final_vector[col] = 0\n",
        "\n",
        "  # default human-readable\n",
        "  hv[\"Estimated Income\"] = \"No data\"\n",
        "\n",
        "  if isinstance(median_income, int):\n",
        "      if median_income < 10000:\n",
        "          bucket = 'HHFAMINC_Less than $10,000'\n",
        "      elif median_income <= 14999:\n",
        "          bucket = 'HHFAMINC_$10,000 to $14,999'\n",
        "\n",
        "      elif median_income <= 24999:\n",
        "          bucket = 'HHFAMINC_$15,000 to $24,999'\n",
        "\n",
        "      elif median_income <= 34999:\n",
        "          bucket = 'HHFAMINC_$25,000 to $34,999'\n",
        "\n",
        "      elif median_income <= 49999:\n",
        "          bucket = 'HHFAMINC_$35,000 to $49,999'\n",
        "\n",
        "      elif median_income <= 74999:\n",
        "          bucket = 'HHFAMINC_$50,000 to $74,999'\n",
        "\n",
        "      elif median_income <= 99999:\n",
        "          bucket = 'HHFAMINC_$75,000 to $99,999'\n",
        "\n",
        "      elif median_income <= 124999:\n",
        "          bucket = 'HHFAMINC_$100,000 to $124,999'\n",
        "\n",
        "      elif median_income <= 149999:\n",
        "          bucket = 'HHFAMINC_$125,000 to $149,999'\n",
        "\n",
        "      elif median_income <= 199999:\n",
        "          bucket = 'HHFAMINC_$150,000 to $199,999'\n",
        "\n",
        "      else:\n",
        "          bucket = 'HHFAMINC_$200,000 or more'\n",
        "\n",
        "\n",
        "      # set the one-hot and human-readable/string fields\n",
        "      if bucket in income_cols:\n",
        "          final_vector[bucket] = 1\n",
        "      hv[\"Estimated Income\"] = bucket[9:]\n",
        "  else:\n",
        "      # leave all income one-hots as 0 and human as No data\n",
        "      final_vector[\"HHFAMINC_No data\"] = 1\n",
        "\n",
        "\n",
        "\n",
        "  # --- MSA Size (one-hot) ---\n",
        "  cbsa_population = get_census_CBSA_data(state_fips, county_fips)\n",
        "\n",
        "  msa_cols = [\n",
        "      'MSASIZE_In an MSA of Less than 250,000',\n",
        "      'MSASIZE_In an MSA of 250,000 - 499,999',\n",
        "      'MSASIZE_In an MSA of 500,000 - 999,999',\n",
        "      'MSASIZE_In an MSA or CMSA of 1,000,000 - 2,999,999',\n",
        "      'MSASIZE_In an MSA or CMSA of 3 million or more',\n",
        "      'MSASIZE_Not in MSA or CMSA'\n",
        "  ]\n",
        "\n",
        "  # initialize\n",
        "  for col in msa_cols:\n",
        "      final_vector[col] = 0\n",
        "\n",
        "  # default text fields\n",
        "  hv[\"Home MSA Size\"] = \"Not in MSA or CMSA\"\n",
        "\n",
        "  if isinstance(cbsa_population, int):\n",
        "      if cbsa_population < 250000:\n",
        "          msa_key = 'MSASIZE_In an MSA of Less than 250,000'\n",
        "\n",
        "      elif cbsa_population <= 499999:\n",
        "          msa_key = 'MSASIZE_In an MSA of 250,000 - 499,999'\n",
        "\n",
        "      elif cbsa_population <= 999999:\n",
        "          msa_key = 'MSASIZE_In an MSA of 500,000 - 999,999'\n",
        "\n",
        "      elif cbsa_population <= 2999999:\n",
        "          msa_key = 'MSASIZE_In an MSA or CMSA of 1,000,000 - 2,999,999'\n",
        "\n",
        "      else:\n",
        "          msa_key = 'MSASIZE_In an MSA or CMSA of 3 million or more'\n",
        "\n",
        "\n",
        "      final_vector[msa_key] = 1\n",
        "      hv[\"Home MSA Size\"] = msa_key[8:]\n",
        "  else:\n",
        "      # If cbsa_population is None or indicates micropolitan / not in MSA\n",
        "      final_vector['MSASIZE_Not in MSA or CMSA'] = 1\n",
        "      hv[\"Home MSA Size\"] = \"Not in MSA or CMSA\"\n",
        "\n",
        "\n",
        "  # --- Urban / Rural (keep existing behavior and human_vector) ---\n",
        "  urban_rural, name = classify_urban(lat, lon)\n",
        "  final_vector[\"URBRUR\"] = 1 if urban_rural == \"Urban\" else 0\n",
        "  hv[\"Home Urban/Rural\"] = urban_rural\n",
        "\n",
        "  #Origin Data\n",
        "  tract_area = get_tract_land_area(state_fips, county_fips, tract)[\"land_area_sqmi\"]\n",
        "  tract_data = get_census_tract_data(state_fips, county_fips, tract)\n",
        "  wdensity = tract_data[\"tract_workers\"]/tract_area\n",
        "  percent_renter_occupied = tract_data[\"percent_renter_occupied\"] * 100\n",
        "  pdensity = tract_data[\"tract_population\"]/tract_area\n",
        "  hdensity = tract_data[\"tract_housing_units\"]/tract_area\n",
        "\n",
        "  #origin renter occupied housing\n",
        "  if percent_renter_occupied < 5:\n",
        "    final_vector[\"OTHTNRNT\"] = 0\n",
        "  elif percent_renter_occupied < 15:\n",
        "    final_vector[\"OTHTNRNT\"] = 5\n",
        "  elif percent_renter_occupied >= 95:\n",
        "    final_vector[\"OTHTNRNT\"] = 95\n",
        "  else:\n",
        "    final_vector[\"OTHTNRNT\"] = (percent_renter_occupied+5) //10 * 10\n",
        "  hv[\"Home Tract % Renter Occupied Housing\"] = percent_renter_occupied\n",
        "\n",
        "  #origin population density\n",
        "\n",
        "  if pdensity < 100:\n",
        "      final_vector[\"OTPPOPDN\"] = 50\n",
        "  elif pdensity < 500:\n",
        "      final_vector[\"OTPPOPDN\"] = 300\n",
        "  elif pdensity < 1000:\n",
        "      final_vector[\"OTPPOPDN\"] = 750\n",
        "  elif pdensity < 2000:\n",
        "      final_vector[\"OTPPOPDN\"] = 1500\n",
        "  elif pdensity < 4000:\n",
        "      final_vector[\"OTPPOPDN\"] = 3000\n",
        "  elif pdensity < 10000:\n",
        "      final_vector[\"OTPPOPDN\"] = 7000\n",
        "  elif pdensity < 25000:\n",
        "      final_vector[\"OTPPOPDN\"] = 17000\n",
        "  else:\n",
        "      final_vector[\"OTPPOPDN\"] = 30000\n",
        "\n",
        "  hv[\"Home Tract Population Density (per sq mile)\"] = pdensity\n",
        "\n",
        "  #origin worker density\n",
        "\n",
        "  if wdensity < 50:\n",
        "      final_vector[\"OTEEMPDN\"] = 25\n",
        "  elif wdensity < 100:\n",
        "      final_vector[\"OTEEMPDN\"] = 75\n",
        "  elif wdensity < 250:\n",
        "      final_vector[\"OTEEMPDN\"] = 150\n",
        "  elif wdensity < 500:\n",
        "      final_vector[\"OTEEMPDN\"] = 350\n",
        "  elif wdensity < 1000:\n",
        "      final_vector[\"OTEEMPDN\"] = 750\n",
        "  elif wdensity < 2000:\n",
        "      final_vector[\"OTEEMPDN\"] = 1500\n",
        "  elif wdensity < 4000:\n",
        "      final_vector[\"OTEEMPDN\"] = 3000\n",
        "  else:\n",
        "      final_vector[\"OTEEMPDN\"] = 5000\n",
        "\n",
        "  hv[\"Home Tract Workers per sq mile\"] = wdensity\n",
        "\n",
        "  #housing unit density\n",
        "\n",
        "  if hdensity < 100:\n",
        "      final_vector[\"OTRESDN\"] = 50\n",
        "  elif hdensity < 500:\n",
        "      final_vector[\"OTRESDN\"] = 300\n",
        "  elif hdensity < 1000:\n",
        "      final_vector[\"OTRESDN\"] = 750\n",
        "  elif hdensity < 2000:\n",
        "      final_vector[\"OTRESDN\"] = 1500\n",
        "  elif hdensity < 4000:\n",
        "      final_vector[\"OTRESDN\"] = 3000\n",
        "  elif hdensity < 10000:\n",
        "      final_vector[\"OTRESDN\"] = 7000\n",
        "  elif hdensity < 25000:\n",
        "      final_vector[\"OTRESDN\"] = 17000\n",
        "  else:\n",
        "      final_vector[\"OTRESDN\"] = 30000\n",
        "\n",
        "  hv[\"Home Tract Housing Units per sq mile\"] = hdensity\n",
        "\n",
        "  #Destination data\n",
        "  if sstate_fips == state_fips and stract == tract:\n",
        "    hv[\"School Tract % Renter Occupied Housing\"] = hv[\"Home Tract % Renter Occupied Housing\"]\n",
        "    hv[\"School Tract Population Density (per sq mile)\"] = hv[\"Home Tract Population Density (per sq mile)\"]\n",
        "    hv[\"School Tract Workers per sq mile\"] = hv[\"Home Tract Workers per sq mile\"]\n",
        "    hv[\"School Tract Housing Units per sq mile\"] = hv[\"Home Tract Housing Units per sq mile\"]\n",
        "\n",
        "  else:\n",
        "    tract_area = get_tract_land_area(sstate_fips, scounty_fips, stract)[\"land_area_sqmi\"]\n",
        "    tract_data = get_census_tract_data(sstate_fips, scounty_fips, stract)\n",
        "\n",
        "    wdensity = tract_data[\"tract_workers\"] / tract_area\n",
        "    percent_renter_occupied = tract_data[\"percent_renter_occupied\"] * 100\n",
        "    pdensity = tract_data[\"tract_population\"] / tract_area\n",
        "    hdensity = tract_data[\"tract_housing_units\"] / tract_area\n",
        "\n",
        "\n",
        "    # ---- Destination renter-occupied housing (D THTNRNT) ----\n",
        "    if percent_renter_occupied < 5:\n",
        "        final_vector[\"DTHTNRNT\"] = 0\n",
        "    elif percent_renter_occupied < 15:\n",
        "        final_vector[\"DTHTNRNT\"] = 5\n",
        "    elif percent_renter_occupied >= 95:\n",
        "        final_vector[\"DTHTNRNT\"] = 95\n",
        "    else:\n",
        "        final_vector[\"DTHTNRNT\"] = (percent_renter_occupied + 5) // 10 * 10\n",
        "\n",
        "    hv[\"School Tract % Renter Occupied Housing\"] = percent_renter_occupied\n",
        "\n",
        "\n",
        "    # ---- Destination population density (D TPPOPDN) ----\n",
        "    if pdensity < 100:\n",
        "        final_vector[\"DTPPOPDN\"] = 50\n",
        "    elif pdensity < 500:\n",
        "        final_vector[\"DTPPOPDN\"] = 300\n",
        "    elif pdensity < 1000:\n",
        "        final_vector[\"DTPPOPDN\"] = 750\n",
        "    elif pdensity < 2000:\n",
        "        final_vector[\"DTPPOPDN\"] = 1500\n",
        "    elif pdensity < 4000:\n",
        "        final_vector[\"DTPPOPDN\"] = 3000\n",
        "    elif pdensity < 10000:\n",
        "        final_vector[\"DTPPOPDN\"] = 7000\n",
        "    elif pdensity < 25000:\n",
        "        final_vector[\"DTPPOPDN\"] = 17000\n",
        "    else:\n",
        "        final_vector[\"DTPPOPDN\"] = 30000\n",
        "\n",
        "    hv[\"School Tract Population Density (per sq mile)\"] = pdensity\n",
        "\n",
        "\n",
        "    # ---- Destination worker density (D TEEMPDN) ----\n",
        "    if wdensity < 50:\n",
        "        final_vector[\"DTEEMPDN\"] = 25\n",
        "    elif wdensity < 100:\n",
        "        final_vector[\"DTEEMPDN\"] = 75\n",
        "    elif wdensity < 250:\n",
        "        final_vector[\"DTEEMPDN\"] = 150\n",
        "    elif wdensity < 500:\n",
        "        final_vector[\"DTEEMPDN\"] = 350\n",
        "    elif wdensity < 1000:\n",
        "        final_vector[\"DTEEMPDN\"] = 750\n",
        "    elif wdensity < 2000:\n",
        "        final_vector[\"DTEEMPDN\"] = 1500\n",
        "    elif wdensity < 4000:\n",
        "        final_vector[\"DTEEMPDN\"] = 3000\n",
        "    else:\n",
        "        final_vector[\"DTEEMPDN\"] = 5000\n",
        "\n",
        "    hv[\"School Tract Workers per sq mile\"] = wdensity\n",
        "\n",
        "\n",
        "    # ---- Destination housing units density (D TRESDN) ----\n",
        "    if hdensity < 100:\n",
        "        final_vector[\"DTRESDN\"] = 50\n",
        "    elif hdensity < 500:\n",
        "        final_vector[\"DTRESDN\"] = 300\n",
        "    elif hdensity < 1000:\n",
        "        final_vector[\"DTRESDN\"] = 750\n",
        "    elif hdensity < 2000:\n",
        "        final_vector[\"DTRESDN\"] = 1500\n",
        "    elif hdensity < 4000:\n",
        "        final_vector[\"DTRESDN\"] = 3000\n",
        "    elif hdensity < 10000:\n",
        "        final_vector[\"DTRESDN\"] = 7000\n",
        "    elif hdensity < 25000:\n",
        "        final_vector[\"DTRESDN\"] = 17000\n",
        "    else:\n",
        "        final_vector[\"DTRESDN\"] = 30000\n",
        "\n",
        "    hv[\"School Tract Housing Units per sq mile\"] = hdensity\n",
        "\n",
        "\n",
        "  return(final_vector, hv)\n",
        "\n"
      ],
      "metadata": {
        "id": "_VpbkzBv-_L1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "suuQd_Xuo7Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = vector_generator(\"7535 Northland Avenue, San Ramon, CA\", \"9870 Broadmoor Drive, San Ramon, CA\")\n",
        "for key in y:\n",
        "  print(key, \":\",y[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD9el4MCo7zO",
        "outputId": "4f0a3682-f0c8-458a-c2a4-cdb8142af79d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Home Latitude : 37.732205002212\n",
            "Home Longitude : -121.941776296797\n",
            "Home State FIPS Code : 06\n",
            "Home County FIPS Code : 013\n",
            "Home Tract Number : 345101\n",
            "Home Block Group Number : 2\n",
            "School Latitude : 37.746077113408\n",
            "School Longitude : -121.947375940902\n",
            "School State FIPS Code : 06\n",
            "School County FIPS Code : 013\n",
            "School Tract Number : 345102\n",
            "School Block Group Number : 3\n",
            "Distance to School : 1.635448472\n",
            "Home Census Division : Pacific\n",
            "Home CBSA Code : 41860\n",
            "Block Group Mode Race : White\n",
            "Block Group Mode Hispanic : Not Hispanic\n",
            "Estimated Income : $200,000 or more\n",
            "Home MSA Size : In an MSA or CMSA of 3 million or more\n",
            "Home Urban/Rural : Urban\n",
            "Home Tract % Renter Occupied Housing : 19.254365266635208\n",
            "Home Tract Population Density (per sq mile) : 7940.047961630696\n",
            "Home Tract Workers per sq mile : 4187.050359712231\n",
            "Home Tract Housing Units per sq mile : 2619.904076738609\n",
            "School Tract % Renter Occupied Housing : 12.284644194756554\n",
            "School Tract Population Density (per sq mile) : 6646.627565982404\n",
            "School Tract Workers per sq mile : 3400.2932551319645\n",
            "School Tract Housing Units per sq mile : 1961.8768328445747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_row = pd.DataFrame([x])\n",
        "\n",
        "\n",
        "# Assume:\n",
        "# df_row: your single-row DataFrame\n",
        "# rf_models: dict of trained RandomForestClassifiers keyed by census_d\n",
        "# e.g., rf_models[0], rf_models[1], etc.\n",
        "\n",
        "\n",
        "# 1️⃣ Get census_d from row\n",
        "census_d_value = df_row['CENSUS_D'].iloc[0]\n",
        "\n",
        "# 2️⃣ Get the correct model\n",
        "if census_d_value not in rf_models:\n",
        "    raise ValueError(f\"No RF model found for census_d = {census_d_value}\")\n",
        "\n",
        "model = rf_models[census_d_value]\n",
        "\n",
        "# 3️⃣ Align features\n",
        "trained_columns = model.feature_names_in_\n",
        "\n",
        "for col in trained_columns:\n",
        "    if col not in df_row.columns:\n",
        "        df_row[col] = 0  # fill missing columns with 0\n",
        "        print(\"filled in column\", col)\n",
        "df_row = df_row[trained_columns]\n",
        "df_row.columns\n",
        "\n",
        "# 4️⃣ Predict\n",
        "pred_class = model.predict(df_row)[0]\n",
        "pred_prob = model.predict_proba(df_row)[0, 1]  # probability of class 1\n",
        "\n",
        "print(\"Predicted SCHOOL_BUS_NEED:\", pred_class)\n",
        "print(\"Probability of needing bus:\", pred_prob)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K4OiMntv9cE",
        "outputId": "1d77c5a7-0302-425e-8a6f-3cf1fb544e25"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted SCHOOL_BUS_NEED: 0\n",
            "Probability of needing bus: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap everything into a function"
      ],
      "metadata": {
        "id": "aOQPbgHex5Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_bus_need(home_address, school_address):\n",
        "  vector, human = vector_generator(home_address, school_address)\n",
        "  df_row = pd.DataFrame([vector])\n",
        "\n",
        "  census_d_value = df_row['CENSUS_D'].iloc[0]\n",
        "\n",
        "  if census_d_value not in rf_models:\n",
        "      raise ValueError(f\"No RF model found for census_d = {census_d_value}\")\n",
        "\n",
        "  model = rf_models[census_d_value]\n",
        "\n",
        "  trained_columns = model.feature_names_in_\n",
        "\n",
        "  for col in trained_columns:\n",
        "      if col not in df_row.columns:\n",
        "          df_row[col] = 0  # fill missing columns with 0\n",
        "          print(\"filled in column\", col)\n",
        "  df_row = df_row[trained_columns]\n",
        "  df_row.columns\n",
        "\n",
        "  pred_class = model.predict(df_row)[0]\n",
        "  pred_prob = model.predict_proba(df_row)[0, 1]  # probability of class 1\n",
        "\n",
        "  for key in human:\n",
        "    print(key, \":\", human[key])\n",
        "  print(\"-----PREDICTION-----\")\n",
        "  print(\"Predicted SCHOOL_BUS_NEED:\", pred_class)\n",
        "  print(\"Probability of needing bus:\", pred_prob)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GykQ1eL8x6gs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_bus_need(\"3312 San Leandro St, Oakland, CA\",\"9870 Broadmoor Drive, San Ramon, CA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDcnD7XPyUUz",
        "outputId": "3c280aa0-a4ad-483a-8b77-b10304d4307a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Home Latitude : 37.774910121324\n",
            "Home Longitude : -122.225908727189\n",
            "Home State FIPS Code : 06\n",
            "Home County FIPS Code : 001\n",
            "Home Tract Number : 406100\n",
            "Home Block Group Number : 1\n",
            "School Latitude : 37.746077113408\n",
            "School Longitude : -121.947375940902\n",
            "School State FIPS Code : 06\n",
            "School County FIPS Code : 013\n",
            "School Tract Number : 345102\n",
            "School Block Group Number : 3\n",
            "Distance to School : 24.602749785300002\n",
            "Home Census Division : Pacific\n",
            "Home CBSA Code : 41860\n",
            "Block Group Mode Race : Other\n",
            "Block Group Mode Hispanic : Hispanic\n",
            "Estimated Income : $25,000 to $34,999\n",
            "Home MSA Size : In an MSA or CMSA of 3 million or more\n",
            "Home Urban/Rural : Urban\n",
            "Home Tract % Renter Occupied Housing : 64.9171270718232\n",
            "Home Tract Population Density (per sq mile) : 6936.797752808989\n",
            "Home Tract Workers per sq mile : 3689.606741573034\n",
            "Home Tract Housing Units per sq mile : 2804.7752808988766\n",
            "School Tract % Renter Occupied Housing : 12.284644194756554\n",
            "School Tract Population Density (per sq mile) : 6646.627565982404\n",
            "School Tract Workers per sq mile : 3400.2932551319645\n",
            "School Tract Housing Units per sq mile : 1961.8768328445747\n",
            "-----PREDICTION-----\n",
            "Predicted SCHOOL_BUS_NEED: 0\n",
            "Probability of needing bus: 0.16\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}